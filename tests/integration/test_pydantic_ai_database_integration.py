"""
Integration test for Pydantic AI database integration.

This test validates the complete pipeline from structured LLM responses
through database storage to JSONB querying with real OpenAI API calls.
"""

import asyncio
import os
import pytest
from typing import Any, AsyncGenerator, Union, Iterator
from uuid import uuid4
from datetime import datetime, timezone

from cognivault.config.openai_config import OpenAIConfig
from cognivault.llm.openai import OpenAIChatLLM
from cognivault.llm.llm_interface import LLMInterface, LLMResponse
from cognivault.agents.critic.agent import CriticAgent
from cognivault.agents.models import CriticOutput, ProcessingMode, ConfidenceLevel
from cognivault.context import AgentContext
from tests.factories.agent_context_factories import (
    AgentContextFactory,
    AgentContextPatterns,
)
from cognivault.database.session_factory import DatabaseSessionFactory
from cognivault.database.repositories.question_repository import QuestionRepository
from cognivault.exceptions import LLMValidationError


class MockStructuredLLM(LLMInterface):
    """Mock LLM that provides realistic structured responses for testing."""

    def __init__(self) -> None:
        self.model = "mock-gpt-4"
        # Flag to indicate this is a mock LLM
        self._is_mock = True
        # Set fake API key for mock mode
        self.api_key = "mock-api-key-for-testing"

    def generate(
        self,
        prompt: Any,
        *,
        stream: bool = False,
        on_log: Any = None,
        system_prompt: Any = None,
        **kwargs: Any,
    ) -> Union[LLMResponse, Iterator[str]]:
        """Generate mock responses that work with structured LLM calls."""
        if on_log:
            on_log(f"[MockLLM] Processing prompt: {prompt[:50]}...")

        # Provide realistic critique responses based on the prompt
        if "AI will completely replace human workers" in prompt:
            response_text = """The statement "AI will completely replace human workers within 5 years" contains several problematic assumptions and overgeneralizations that warrant critical examination."""
        elif "edge case" in prompt.lower() or "test query" in prompt.lower():
            response_text = """This appears to be a test query with potential edge case scenarios that may challenge typical response patterns."""
        else:
            response_text = f"This query requires careful analysis to identify potential issues or areas for improvement: {prompt[:100]}"

        return LLMResponse(
            text=response_text,
            tokens_used=150,
            model_name=self.model,
            finish_reason="stop",
        )


class TestPydanticAIDatabaseIntegration:
    """
    Integration tests for the complete Pydantic AI + Database pipeline.

    These tests require:
    - Valid OpenAI API key in environment
    - PostgreSQL database running
    - All Pydantic AI infrastructure operational
    """

    @pytest.fixture
    async def db_session_manager(self) -> AsyncGenerator[Any, None]:
        """Setup database session manager for testing."""
        session_manager = DatabaseSessionFactory()
        try:
            await session_manager.initialize()
            yield session_manager
        except Exception as e:
            pytest.skip(f"Database not available for integration testing: {e}")
        finally:
            if session_manager.is_initialized:
                await session_manager.shutdown()

    @pytest.fixture
    async def question_repository(
        self, db_session_manager: Any
    ) -> AsyncGenerator[Any, None]:
        """Setup question repository with database session."""
        async with db_session_manager.get_session() as session:
            yield QuestionRepository(session)

    @pytest.fixture
    def openai_llm(self) -> Any:
        """Setup LLM - use real OpenAI if API key available, otherwise use mock."""
        # Check if we have a valid OpenAI API key
        api_key = os.getenv("OPENAI_API_KEY")

        # Check for test/fake API keys and use mock instead
        if (
            not api_key
            or api_key.startswith("test-key")
            or "safe-for-testing" in api_key
            or len(api_key) < 20
        ):
            print("🔧 Using MockStructuredLLM for testing (no valid OpenAI API key)")
            return MockStructuredLLM()

        # Only use real OpenAI for valid production keys
        if api_key.startswith("sk-") and len(api_key) > 40:
            try:
                config = OpenAIConfig.load()
                return OpenAIChatLLM(api_key=config.api_key, model=config.model)
            except Exception:
                # Fall back to mock if config loading fails
                pass

        # Default fallback to mock
        print("🔧 Using MockStructuredLLM for testing (no valid OpenAI API key)")
        return MockStructuredLLM()

    @pytest.fixture
    def critic_agent(self, openai_llm: Any) -> Any:
        """Setup CriticAgent with real LLM for structured responses."""
        return CriticAgent(openai_llm)

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_end_to_end_structured_llm_database_pipeline(
        self, critic_agent: Any, question_repository: Any
    ) -> None:
        """
        Test complete pipeline: LLM structured response → database storage → querying.

        This integration test validates:
        1. Structured LLM response generation with Pydantic AI
        2. Database storage of structured metadata in JSONB
        3. Querying structured data using repository helper methods
        4. Data consistency and type validation throughout pipeline
        """
        # Test query that should generate interesting critique
        test_query = "AI will replace all jobs soon"
        correlation_id = f"test-integration-{uuid4().hex[:8]}"

        print(f"\n🔄 Testing end-to-end pipeline with query: '{test_query}'")

        # Step 1: Execute structured agent with real LLM
        context = AgentContextFactory.basic(
            query=test_query, correlation_id=correlation_id
        )
        context.add_agent_output("refiner", test_query)  # Simulate refiner output

        print("📡 Calling real OpenAI API for structured response...")
        enhanced_context = await critic_agent.run_structured(context)

        # Validate structured output was created
        assert "structured_outputs" in enhanced_context.execution_state
        assert "critic" in enhanced_context.execution_state["structured_outputs"]

        structured_output = enhanced_context.execution_state["structured_outputs"][
            "critic"
        ]
        assert isinstance(structured_output, CriticOutput)

        print(
            f"✅ Generated structured output: {structured_output.issues_detected} issues, "
            f"{structured_output.confidence} confidence"
        )

        # Step 2: Store in database with structured metadata
        execution_metadata = {
            "execution_id": f"exec-{uuid4().hex[:8]}",
            "correlation_id": correlation_id,
            "total_execution_time_ms": 2500.0,
            "nodes_executed": ["refiner", "critic"],
            "parallel_execution": False,
            "agent_outputs": {
                "critic": structured_output.model_dump()  # Convert Pydantic model to dict
            },
            "total_tokens_used": 1500,
            "total_cost_usd": 0.03,
            "model_used": "gpt-4",
            "success": True,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

        print("💾 Storing structured data in PostgreSQL JSONB...")
        question = await question_repository.create_question(
            query=test_query,
            correlation_id=correlation_id,
            execution_id=execution_metadata["execution_id"],
            nodes_executed=execution_metadata["nodes_executed"],
            execution_metadata=execution_metadata,
        )

        assert question is not None
        assert question.execution_metadata is not None
        assert "agent_outputs" in question.execution_metadata
        assert "critic" in question.execution_metadata["agent_outputs"]

        print(f"✅ Stored question with ID: {question.id}")

        # Step 3: Query structured data using repository helpers
        print("🔍 Testing JSONB query helper methods...")

        # Test confidence-based querying
        confidence_questions = (
            await question_repository.get_questions_by_agent_confidence(
                agent_name="critic",
                confidence_level=structured_output.confidence,
                limit=10,
            )
        )

        assert len(confidence_questions) >= 1
        found_question = next(
            (q for q in confidence_questions if q.correlation_id == correlation_id),
            None,
        )
        assert found_question is not None

        print(
            f"✅ Found {len(confidence_questions)} questions with {structured_output.confidence} confidence"
        )

        # Test issue detection querying
        if structured_output.issues_detected > 0:
            issues_questions = await question_repository.get_questions_with_issues(
                min_issues=structured_output.issues_detected
            )
            assert len(issues_questions) >= 1
            print(
                f"✅ Found {len(issues_questions)} questions with {structured_output.issues_detected}+ issues"
            )

        # Test agent performance statistics
        performance_stats = await question_repository.get_agent_performance_stats(
            "critic"
        )
        assert performance_stats["total_executions"] >= 1
        assert performance_stats["agent_name"] == "critic"
        assert "confidence_distribution" in performance_stats
        assert (
            structured_output.confidence in performance_stats["confidence_distribution"]
        )

        print(
            f"✅ Agent performance stats: {performance_stats['total_executions']} executions"
        )

        # Test structured output querying
        structured_questions = (
            await question_repository.get_questions_with_structured_outputs(limit=10)
        )
        assert len(structured_questions) >= 1

        found_structured = next(
            (q for q in structured_questions if q.correlation_id == correlation_id),
            None,
        )
        assert found_structured is not None

        print(f"✅ Found {len(structured_questions)} questions with structured outputs")

        # Step 4: Validate data consistency and types
        stored_critic_output = question.execution_metadata["agent_outputs"]["critic"]

        # Validate all required Pydantic fields are present
        required_fields = [
            "agent_name",
            "processing_mode",
            "confidence",
            "critique_summary",
            "issues_detected",
        ]
        for field in required_fields:
            assert field in stored_critic_output, f"Missing required field: {field}"

        # Validate field types match Pydantic model expectations
        assert isinstance(stored_critic_output["issues_detected"], int)
        assert stored_critic_output["confidence"] in ["high", "medium", "low"]
        assert stored_critic_output["processing_mode"] in [
            "active",
            "passive",
            "fallback",
        ]
        assert isinstance(stored_critic_output["critique_summary"], str)
        assert len(stored_critic_output["critique_summary"]) > 0

        print("✅ Data consistency validation passed")

        # Step 5: Test execution time statistics
        time_stats = await question_repository.get_execution_time_statistics()
        assert time_stats["total_questions"] >= 1
        assert "avg_total_time_ms" in time_stats

        print(
            f"✅ Execution time statistics: {time_stats['total_questions']} questions analyzed"
        )

        print(
            "\n🎉 End-to-end Pydantic AI database integration test completed successfully!"
        )
        print(
            f"   ✓ Structured LLM response generated with {structured_output.issues_detected} issues"
        )
        print(f"   ✓ Data stored in PostgreSQL JSONB format")
        print(f"   ✓ JSONB queries working with helper methods")
        print(f"   ✓ Data consistency and type validation passed")

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_structured_llm_fallback_behavior(
        self, critic_agent: Any, question_repository: Any
    ) -> None:
        """
        Test that structured LLM gracefully falls back to unstructured when validation fails.

        This test validates the resilience of the structured pipeline when:
        - LLM returns malformed JSON
        - Pydantic validation fails
        - Network issues occur during structured calls
        """
        # Test with a query that might produce edge case responses
        edge_case_query = "Quantum consciousness emergence singularity nexus"
        correlation_id = f"test-fallback-{uuid4().hex[:8]}"

        print(
            f"\n🔄 Testing fallback behavior with edge case query: '{edge_case_query}'"
        )

        context = AgentContextFactory.basic(
            query=edge_case_query, correlation_id=correlation_id
        )
        context.add_agent_output("refiner", edge_case_query)

        try:
            # This should either succeed with structured output or fallback gracefully
            enhanced_context = await critic_agent.run_structured(context)

            # Validate that we got some form of output (structured or fallback)
            assert (
                "critic" in enhanced_context.agent_outputs
            )  # Basic output should always exist

            if (
                "structured_outputs" in enhanced_context.execution_state
                and "critic" in enhanced_context.execution_state["structured_outputs"]
            ):
                structured_output = enhanced_context.execution_state[
                    "structured_outputs"
                ]["critic"]
                assert isinstance(structured_output, CriticOutput)
                print(
                    f"✅ Structured response succeeded: {structured_output.confidence} confidence"
                )
            else:
                print("✅ Graceful fallback to unstructured response")

            # Store the result regardless of structured/unstructured
            execution_metadata = {
                "execution_id": f"exec-fallback-{uuid4().hex[:8]}",
                "correlation_id": correlation_id,
                "total_execution_time_ms": 1800.0,
                "nodes_executed": ["refiner", "critic"],
                "agent_outputs": {
                    "critic": (
                        enhanced_context.execution_state["structured_outputs"]
                        .get("critic", {})
                        .model_dump()
                        if "structured_outputs" in enhanced_context.execution_state
                        and "critic"
                        in enhanced_context.execution_state["structured_outputs"]
                        else {
                            "fallback_response": enhanced_context.agent_outputs[
                                "critic"
                            ]
                        }
                    )
                },
                "success": True,
                "fallback_used": not (
                    "structured_outputs" in enhanced_context.execution_state
                    and "critic"
                    in enhanced_context.execution_state["structured_outputs"]
                ),
            }

            question = await question_repository.create_question(
                query=edge_case_query,
                correlation_id=correlation_id,
                execution_metadata=execution_metadata,
            )

            assert question is not None
            print("✅ Fallback result stored successfully")

        except LLMValidationError as e:
            print(f"✅ LLMValidationError properly raised and handled: {e.message}")
            # This is expected behavior - the system should handle validation failures gracefully

        except Exception as e:
            pytest.fail(f"Unexpected exception in fallback test: {e}")

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_pydantic_ai_performance_benchmarking(
        self, critic_agent: Any, question_repository: Any
    ) -> None:
        """
        Performance test comparing structured vs unstructured LLM calls.

        This test measures:
        - Response time differences between structured and unstructured calls
        - Success rates for structured parsing
        - Resource usage patterns
        """
        test_queries = [
            "What is machine learning?",
            "How does blockchain work?",
            "Explain quantum computing",
            "What are the benefits of renewable energy?",
            "How does artificial intelligence impact society?",
        ]

        print(f"\n📊 Performance benchmarking: {len(test_queries)} queries")

        structured_times = []
        unstructured_times = []
        structured_success_count = 0

        for i, query in enumerate(test_queries):
            correlation_id = f"perf-test-{i}-{uuid4().hex[:6]}"
            context = AgentContextFactory.basic(
                query=query, correlation_id=correlation_id
            )
            context.add_agent_output("refiner", query)

            # Test structured call
            start_time = asyncio.get_event_loop().time()
            try:
                structured_context = await critic_agent.run_structured(context)
                structured_time = asyncio.get_event_loop().time() - start_time
                structured_times.append(structured_time)

                if (
                    "structured_outputs" in structured_context.execution_state
                    and "critic"
                    in structured_context.execution_state["structured_outputs"]
                ):
                    structured_success_count += 1

            except Exception as e:
                print(f"⚠️  Structured call failed for query {i}: {e}")
                structured_times.append(float("inf"))  # Mark as failed

            # Test unstructured call for comparison
            context_unstructured = AgentContextFactory.basic(
                query=query, correlation_id=f"{correlation_id}-unstructured"
            )
            context_unstructured.add_agent_output("refiner", query)

            start_time = asyncio.get_event_loop().time()
            try:
                await critic_agent.run(context_unstructured)
                unstructured_time = asyncio.get_event_loop().time() - start_time
                unstructured_times.append(unstructured_time)
            except Exception as e:
                print(f"⚠️  Unstructured call failed for query {i}: {e}")
                unstructured_times.append(float("inf"))

        # Calculate performance metrics
        valid_structured_times = [t for t in structured_times if t != float("inf")]
        valid_unstructured_times = [t for t in unstructured_times if t != float("inf")]

        if valid_structured_times and valid_unstructured_times:
            avg_structured = sum(valid_structured_times) / len(valid_structured_times)
            avg_unstructured = sum(valid_unstructured_times) / len(
                valid_unstructured_times
            )

            structured_success_rate = structured_success_count / len(test_queries) * 100

            print(f"\n📈 Performance Results:")
            print(f"   Structured avg time: {avg_structured:.2f}s")
            print(f"   Unstructured avg time: {avg_unstructured:.2f}s")
            print(f"   Structured success rate: {structured_success_rate:.1f}%")
            print(
                f"   Overhead: {((avg_structured - avg_unstructured) / avg_unstructured * 100):+.1f}%"
            )

            # Basic performance assertions
            assert structured_success_rate >= 60, (
                f"Structured success rate too low: {structured_success_rate}%"
            )

            # For mock LLMs, timing might be very different, so use a more lenient assertion
            # Check if we're using mock LLM by looking at the critic agent's LLM type
            using_mock = hasattr(critic_agent, "llm") and isinstance(
                critic_agent.llm, MockStructuredLLM
            )
            max_overhead_multiplier = (
                10 if using_mock else 3
            )  # More lenient for mock LLMs

            # Handle edge case where unstructured time is near zero (mock LLMs)
            if avg_unstructured < 0.001:  # Less than 1ms
                # For extremely fast mock calls, just check that structured isn't excessively slow
                max_absolute_time = (
                    5.0 if using_mock else 1.0
                )  # 5s for mock, 1s for real
                assert avg_structured < max_absolute_time, (
                    f"Structured calls too slow with near-zero unstructured baseline: "
                    f"{avg_structured:.2f}s vs {avg_unstructured:.2f}s "
                    f"[Using {'mock' if using_mock else 'real'} LLM]"
                )
            else:
                # Normal case with meaningful timing comparison
                overhead_percent = (
                    (avg_structured - avg_unstructured) / avg_unstructured * 100
                )
                assert avg_structured < avg_unstructured * max_overhead_multiplier, (
                    f"Structured calls taking too much longer than unstructured: "
                    f"{avg_structured:.2f}s vs {avg_unstructured:.2f}s "
                    f"(overhead: {overhead_percent:+.1f}%) "
                    f"[Using {'mock' if using_mock else 'real'} LLM]"
                )

            print("✅ Performance benchmarking completed")
        else:
            pytest.skip("Insufficient valid timing data for performance comparison")


if __name__ == "__main__":
    """
    Run integration tests directly for development.

    Usage:
        python -m pytest tests/integration/test_pydantic_ai_database_integration.py -v -s

    Or run specific test:
        python -m pytest tests/integration/test_pydantic_ai_database_integration.py::TestPydanticAIDatabaseIntegration::test_end_to_end_structured_llm_database_pipeline -v -s
    """
    pytest.main([__file__, "-v", "-s"])
