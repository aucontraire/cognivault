#!/usr/bin/env python3
"""
CI/CD OpenAI Parameter Regression Prevention Test Suite
Automated tests to prevent re-introduction of parameter compatibility issues in CI/CD pipeline.

PURPOSE:
- Block deployments that could re-introduce 82-second timeout cascades
- Validate parameter transformation code changes don't break compatibility fixes
- Ensure new LLM integrations follow established parameter patterns
- Prevent regression of performance improvements (680ms-2s response times)

CI/CD INTEGRATION:
- Run on every PR that touches LLM-related code
- Block merges if parameter compatibility tests fail
- Validate API parameter usage patterns in codebase
- Performance regression detection with failure thresholds
"""

import pytest
import ast
import os
import re
import json
from pathlib import Path
from typing import Dict, Any, List, Optional, Set, Tuple
from unittest.mock import patch, Mock
import subprocess

# Import code analysis tools
try:
    from cognivault.services.langchain_service import LangChainService
    from cognivault.services.llm_pool import LLMServicePool
    from cognivault.exceptions.llm_errors import LLMParameterError
except ImportError:
    # Handle graceful degradation for CI environments
    pass


class ParameterUsageAnalyzer(ast.NodeVisitor):
    """AST analyzer to detect OpenAI parameter usage patterns in code"""

    def __init__(self):
        self.parameter_usage = []
        self.incompatible_patterns = []
        self.current_file = None
        self.current_function = None

    def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """Analyze a Python file for parameter usage patterns"""
        self.current_file = str(file_path)
        self.parameter_usage = []
        self.incompatible_patterns = []

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                tree = ast.parse(f.read(), filename=str(file_path))
                self.visit(tree)
        except Exception as e:
            return {"error": f"Failed to parse {file_path}: {e}"}

        return {
            "file": self.current_file,
            "parameter_usage": self.parameter_usage,
            "incompatible_patterns": self.incompatible_patterns,
            "issues_found": len(self.incompatible_patterns),
        }

    def visit_FunctionDef(self, node):
        """Track function definitions for context"""
        old_function = self.current_function
        self.current_function = node.name
        self.generic_visit(node)
        self.current_function = old_function

    def visit_Call(self, node):
        """Analyze function calls for OpenAI parameter usage"""
        # Check for OpenAI client calls
        if self._is_openai_call(node):
            self._analyze_openai_call(node)

        self.generic_visit(node)

    def visit_Dict(self, node):
        """Analyze dictionary literals for parameter patterns"""
        # Look for parameter dictionaries
        if self._looks_like_llm_params(node):
            self._analyze_parameter_dict(node)

        self.generic_visit(node)

    def _is_openai_call(self, node: ast.Call) -> bool:
        """Check if this is an OpenAI API call"""
        # Check for various OpenAI call patterns
        openai_patterns = [
            "chat.completions.create",
            "beta.chat.completions.parse",
            "completions.create",
            "client.chat.completions",
        ]

        call_str = self._get_call_string(node)
        return any(pattern in call_str for pattern in openai_patterns)

    def _looks_like_llm_params(self, node: ast.Dict) -> bool:
        """Check if dictionary looks like LLM parameters"""
        if not node.keys:
            return False

        # Look for common LLM parameter keys
        llm_param_keys = {
            "model",
            "messages",
            "max_tokens",
            "max_completion_tokens",
            "temperature",
            "response_format",
        }

        dict_keys = set()
        for key in node.keys:
            if isinstance(key, ast.Constant) and isinstance(key.value, str):
                dict_keys.add(key.value)

        return len(dict_keys.intersection(llm_param_keys)) >= 2

    def _analyze_openai_call(self, node: ast.Call):
        """Analyze OpenAI API call for parameter compatibility"""
        call_info = {
            "type": "openai_call",
            "function": self.current_function,
            "line": node.lineno,
            "call_pattern": self._get_call_string(node),
            "parameters": {},
        }

        # Extract keyword arguments
        for keyword in node.keywords:
            if keyword.arg:
                param_value = self._extract_value(keyword.value)
                call_info["parameters"][keyword.arg] = param_value

                # Check for incompatible patterns
                self._check_parameter_compatibility(
                    keyword.arg, param_value, node.lineno
                )

        self.parameter_usage.append(call_info)

    def _analyze_parameter_dict(self, node: ast.Dict):
        """Analyze parameter dictionary for compatibility issues"""
        dict_info = {
            "type": "parameter_dict",
            "function": self.current_function,
            "line": node.lineno,
            "parameters": {},
        }

        for i, (key, value) in enumerate(zip(node.keys, node.values)):
            if isinstance(key, ast.Constant) and isinstance(key.value, str):
                param_name = key.value
                param_value = self._extract_value(value)
                dict_info["parameters"][param_name] = param_value

                # Check for incompatible patterns
                self._check_parameter_compatibility(
                    param_name, param_value, node.lineno
                )

        self.parameter_usage.append(dict_info)

    def _check_parameter_compatibility(
        self, param_name: str, param_value: Any, line_number: int
    ):
        """Check individual parameter for compatibility issues"""

        # Rule 1: max_tokens usage with GPT-5 models
        if param_name == "max_tokens":
            self.incompatible_patterns.append(
                {
                    "issue": "max_tokens_usage",
                    "description": "max_tokens parameter may cause issues with GPT-5 models",
                    "recommendation": "Use max_completion_tokens for GPT-5 compatibility",
                    "line": line_number,
                    "parameter": param_name,
                    "value": param_value,
                    "severity": "HIGH",
                }
            )

        # Rule 2: Temperature values other than 1.0 for GPT-5
        if (
            param_name == "temperature"
            and param_value != 1.0
            and param_value is not None
        ):
            # This is a warning since we can't always detect the model context
            self.incompatible_patterns.append(
                {
                    "issue": "temperature_compatibility",
                    "description": f"Temperature {param_value} may not work with GPT-5 models (only supports 1.0)",
                    "recommendation": "Use conditional temperature setting based on model type",
                    "line": line_number,
                    "parameter": param_name,
                    "value": param_value,
                    "severity": "MEDIUM",
                }
            )

        # Rule 3: Unsupported parameters for GPT-5
        gpt5_unsupported = [
            "top_p",
            "frequency_penalty",
            "presence_penalty",
            "logit_bias",
        ]
        if param_name in gpt5_unsupported:
            self.incompatible_patterns.append(
                {
                    "issue": "unsupported_parameter",
                    "description": f"Parameter {param_name} not supported by GPT-5 models",
                    "recommendation": f"Use conditional parameter setting to exclude {param_name} for GPT-5",
                    "line": line_number,
                    "parameter": param_name,
                    "value": param_value,
                    "severity": "MEDIUM",
                }
            )

    def _get_call_string(self, node: ast.Call) -> str:
        """Get string representation of function call"""
        try:
            if isinstance(node.func, ast.Attribute):
                # Handle method calls like client.chat.completions.create
                parts = []
                current = node.func
                while isinstance(current, ast.Attribute):
                    parts.append(current.attr)
                    current = current.value
                if isinstance(current, ast.Name):
                    parts.append(current.id)
                return ".".join(reversed(parts))
            elif isinstance(node.func, ast.Name):
                return node.func.id
        except:
            pass
        return "unknown_call"

    def _extract_value(self, node: ast.AST) -> Any:
        """Extract value from AST node"""
        if isinstance(node, ast.Constant):
            return node.value
        elif isinstance(node, ast.Name):
            return f"variable:{node.id}"
        elif isinstance(node, ast.Attribute):
            return f"attribute:{self._get_attribute_string(node)}"
        else:
            return f"expression:{type(node).__name__}"

    def _get_attribute_string(self, node: ast.Attribute) -> str:
        """Get string representation of attribute access"""
        if isinstance(node.value, ast.Name):
            return f"{node.value.id}.{node.attr}"
        elif isinstance(node.value, ast.Attribute):
            return f"{self._get_attribute_string(node.value)}.{node.attr}"
        else:
            return f"unknown.{node.attr}"


class CIParameterRegressionPrevention:
    """CI/CD regression prevention framework for OpenAI parameters"""

    def __init__(self, repo_root: Path = None):
        self.repo_root = repo_root or Path.cwd()
        self.analyzer = ParameterUsageAnalyzer()

        # File patterns to scan for parameter usage
        self.scan_patterns = [
            "src/cognivault/**/*.py",
            "tests/**/*.py",
            "scripts/**/*.py",
        ]

        # Files to exclude from scanning
        self.exclude_patterns = [
            "**/test_openai_parameter*.py",  # Exclude our own test files
            "**/__pycache__/**",
            "**/.*/**",
            "**/build/**",
            "**/dist/**",
        ]

    def scan_codebase_for_parameter_issues(self) -> Dict[str, Any]:
        """Scan entire codebase for parameter compatibility issues"""
        print("üîç Scanning codebase for OpenAI parameter compatibility issues...")

        results = {
            "scan_timestamp": __import__("time").time(),
            "files_scanned": 0,
            "files_with_issues": 0,
            "total_issues": 0,
            "issues_by_severity": {"HIGH": 0, "MEDIUM": 0, "LOW": 0},
            "file_results": [],
            "summary": {},
        }

        # Find files to scan
        files_to_scan = self._find_files_to_scan()

        for file_path in files_to_scan:
            results["files_scanned"] += 1

            # Analyze file
            analysis = self.analyzer.analyze_file(file_path)

            if analysis.get("issues_found", 0) > 0:
                results["files_with_issues"] += 1
                results["total_issues"] += analysis["issues_found"]

                # Count by severity
                for issue in analysis.get("incompatible_patterns", []):
                    severity = issue.get("severity", "LOW")
                    results["issues_by_severity"][severity] = (
                        results["issues_by_severity"].get(severity, 0) + 1
                    )

                results["file_results"].append(analysis)
                print(
                    f"  ‚ö†Ô∏è  {file_path.relative_to(self.repo_root)}: {analysis['issues_found']} issues"
                )

            if results["files_scanned"] % 50 == 0:
                print(f"    Progress: {results['files_scanned']} files scanned...")

        # Generate summary
        results["summary"] = {
            "files_scanned": results["files_scanned"],
            "files_with_issues": results["files_with_issues"],
            "issue_rate": (
                results["files_with_issues"] / results["files_scanned"]
                if results["files_scanned"] > 0
                else 0
            ),
            "critical_issues": results["issues_by_severity"]["HIGH"],
            "warning_issues": results["issues_by_severity"]["MEDIUM"],
            "info_issues": results["issues_by_severity"]["LOW"],
            "blocks_deployment": results["issues_by_severity"]["HIGH"] > 0,
        }

        return results

    def _find_files_to_scan(self) -> List[Path]:
        """Find Python files to scan based on patterns"""
        files = []

        for pattern in self.scan_patterns:
            for file_path in self.repo_root.glob(pattern):
                if file_path.is_file() and file_path.suffix == ".py":
                    # Check if file should be excluded
                    should_exclude = False
                    for exclude_pattern in self.exclude_patterns:
                        if file_path.match(exclude_pattern):
                            should_exclude = True
                            break

                    if not should_exclude:
                        files.append(file_path)

        return sorted(set(files))  # Remove duplicates and sort

    def validate_parameter_transformation_functions(self) -> Dict[str, Any]:
        """Validate that parameter transformation functions work correctly"""
        print("üîß Validating parameter transformation functions...")

        validation_results = {
            "transformation_tests": [],
            "all_tests_passed": True,
            "critical_failures": [],
        }

        # Test cases for parameter transformation
        test_cases = [
            {
                "name": "max_tokens_to_max_completion_tokens_gpt5",
                "input_params": {
                    "model": "gpt-5-nano",
                    "max_tokens": 150,
                    "temperature": 0.7,
                },
                "expected_params": {
                    "model": "gpt-5-nano",
                    "max_completion_tokens": 150,
                    "temperature": 1.0,
                },
                "model_pattern": "gpt-5",
            },
            {
                "name": "preserve_max_tokens_for_gpt4",
                "input_params": {
                    "model": "gpt-4o",
                    "max_tokens": 150,
                    "temperature": 0.7,
                },
                "expected_params": {
                    "model": "gpt-4o",
                    "max_tokens": 150,
                    "temperature": 0.7,
                },
                "model_pattern": "gpt-4",
            },
            {
                "name": "temperature_filtering_gpt5",
                "input_params": {"model": "gpt-5", "temperature": 0.5},
                "expected_params": {"model": "gpt-5", "temperature": 1.0},
                "model_pattern": "gpt-5",
            },
            {
                "name": "remove_unsupported_params_gpt5",
                "input_params": {
                    "model": "gpt-5-nano",
                    "top_p": 0.9,
                    "frequency_penalty": 0.1,
                },
                "expected_params": {"model": "gpt-5-nano"},
                "model_pattern": "gpt-5",
            },
        ]

        for test_case in test_cases:
            test_result = self._test_parameter_transformation(test_case)
            validation_results["transformation_tests"].append(test_result)

            if not test_result["passed"]:
                validation_results["all_tests_passed"] = False
                if test_case["model_pattern"] == "gpt-5":
                    validation_results["critical_failures"].append(test_result)

        return validation_results

    def _test_parameter_transformation(
        self, test_case: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Test a single parameter transformation case"""

        # Mock parameter transformation function
        def mock_transform_parameters_for_model(
            params: Dict[str, Any],
        ) -> Dict[str, Any]:
            """Mock the actual parameter transformation logic"""
            transformed = params.copy()
            model = params.get("model", "")

            if "gpt-5" in model.lower():
                # Apply GPT-5 compatibility fixes
                if "max_tokens" in transformed:
                    transformed["max_completion_tokens"] = transformed.pop("max_tokens")

                # Force temperature to 1.0
                if "temperature" in transformed:
                    transformed["temperature"] = 1.0

                # Remove unsupported parameters
                unsupported = [
                    "top_p",
                    "frequency_penalty",
                    "presence_penalty",
                    "logit_bias",
                ]
                for param in unsupported:
                    transformed.pop(param, None)

            return transformed

        try:
            # Apply transformation
            result_params = mock_transform_parameters_for_model(
                test_case["input_params"]
            )

            # Check if result matches expected
            expected = test_case["expected_params"]
            passed = True
            differences = []

            # Check all expected parameters are present with correct values
            for key, expected_value in expected.items():
                if key not in result_params:
                    passed = False
                    differences.append(f"Missing parameter: {key}")
                elif result_params[key] != expected_value:
                    passed = False
                    differences.append(
                        f"Parameter {key}: expected {expected_value}, got {result_params[key]}"
                    )

            # Check no unexpected parameters are present (for removal tests)
            for key in result_params:
                if key not in expected:
                    # Allow model parameter to always be present
                    if key != "model":
                        passed = False
                        differences.append(
                            f"Unexpected parameter: {key} = {result_params[key]}"
                        )

            return {
                "test_name": test_case["name"],
                "passed": passed,
                "input_params": test_case["input_params"],
                "expected_params": expected,
                "actual_params": result_params,
                "differences": differences,
            }

        except Exception as e:
            return {
                "test_name": test_case["name"],
                "passed": False,
                "input_params": test_case["input_params"],
                "expected_params": test_case["expected_params"],
                "actual_params": {},
                "error": str(e),
                "differences": [f"Transformation failed: {e}"],
            }

    def check_git_changes_for_parameter_impact(self) -> Dict[str, Any]:
        """Check git changes for potential parameter compatibility impact"""
        print("üìù Analyzing git changes for parameter compatibility impact...")

        try:
            # Get changed files (staged and unstaged)
            result = subprocess.run(
                ["git", "diff", "--name-only", "HEAD"],
                capture_output=True,
                text=True,
                cwd=self.repo_root,
            )

            if result.returncode != 0:
                return {"error": "Failed to get git changes", "git_available": False}

            changed_files = [
                line.strip() for line in result.stdout.split("\n") if line.strip()
            ]

            # Filter for Python files
            python_files = [f for f in changed_files if f.endswith(".py")]

            analysis = {
                "git_available": True,
                "total_changed_files": len(changed_files),
                "python_files_changed": len(python_files),
                "files_requiring_analysis": [],
                "risk_assessment": "LOW",
            }

            # Analyze changed Python files
            high_risk_patterns = [
                r"langchain.*service",
                r"llm.*pool",
                r"openai.*client",
                r"parameter.*transform",
                r"chat\.completions",
                r"max_tokens",
                r"temperature",
            ]

            for file_path in python_files:
                file_risk = "LOW"
                risk_indicators = []

                # Check file path for high-risk patterns
                for pattern in high_risk_patterns:
                    if re.search(pattern, file_path, re.IGNORECASE):
                        file_risk = "HIGH"
                        risk_indicators.append(f"File path matches pattern: {pattern}")
                        analysis["risk_assessment"] = "HIGH"

                # Try to analyze file content changes
                try:
                    content_result = subprocess.run(
                        ["git", "diff", "HEAD", "--", file_path],
                        capture_output=True,
                        text=True,
                        cwd=self.repo_root,
                    )

                    if content_result.returncode == 0:
                        diff_content = content_result.stdout

                        # Check diff content for parameter-related changes
                        parameter_keywords = [
                            "max_tokens",
                            "max_completion_tokens",
                            "temperature",
                            "openai",
                            "gpt-5",
                            "chat.completions",
                            "response_format",
                        ]

                        for keyword in parameter_keywords:
                            if keyword in diff_content.lower():
                                if file_risk == "LOW":
                                    file_risk = "MEDIUM"
                                    if analysis["risk_assessment"] == "LOW":
                                        analysis["risk_assessment"] = "MEDIUM"
                                risk_indicators.append(
                                    f"Content change involves: {keyword}"
                                )

                except subprocess.SubprocessError:
                    pass  # Skip content analysis if git diff fails

                if file_risk != "LOW":
                    analysis["files_requiring_analysis"].append(
                        {
                            "file": file_path,
                            "risk_level": file_risk,
                            "risk_indicators": risk_indicators,
                        }
                    )

            return analysis

        except subprocess.SubprocessError:
            return {"error": "Git not available", "git_available": False}

    def generate_ci_report(self) -> Dict[str, Any]:
        """Generate comprehensive CI/CD report for parameter compatibility"""
        print("\nüìä Generating CI/CD Parameter Compatibility Report...")

        # Run all analyses
        codebase_scan = self.scan_codebase_for_parameter_issues()
        transformation_validation = self.validate_parameter_transformation_functions()
        git_analysis = self.check_git_changes_for_parameter_impact()

        # Determine overall status
        blocks_deployment = (
            codebase_scan["summary"]["blocks_deployment"]
            or not transformation_validation["all_tests_passed"]
            or git_analysis.get("risk_assessment") == "HIGH"
        )

        report = {
            "timestamp": __import__("time").time(),
            "overall_status": "FAIL" if blocks_deployment else "PASS",
            "blocks_deployment": blocks_deployment,
            "analyses": {
                "codebase_scan": codebase_scan,
                "transformation_validation": transformation_validation,
                "git_analysis": git_analysis,
            },
            "summary": {
                "critical_issues_found": codebase_scan["summary"]["critical_issues"],
                "transformation_tests_passed": transformation_validation[
                    "all_tests_passed"
                ],
                "git_risk_level": git_analysis.get("risk_assessment", "UNKNOWN"),
                "recommendation": "",
            },
        }

        # Generate recommendation
        if blocks_deployment:
            recommendations = []
            if codebase_scan["summary"]["critical_issues"] > 0:
                recommendations.append(
                    f"Fix {codebase_scan['summary']['critical_issues']} critical parameter compatibility issues"
                )
            if not transformation_validation["all_tests_passed"]:
                recommendations.append("Fix parameter transformation function failures")
            if git_analysis.get("risk_assessment") == "HIGH":
                recommendations.append(
                    "Review high-risk parameter-related code changes"
                )

            report["summary"]["recommendation"] = "BLOCK DEPLOYMENT: " + "; ".join(
                recommendations
            )
        else:
            report["summary"]["recommendation"] = (
                "ALLOW DEPLOYMENT: All parameter compatibility checks passed"
            )

        return report


class TestCIParameterRegressionPrevention:
    """CI/CD test suite for parameter compatibility regression prevention"""

    @pytest.fixture
    def ci_framework(self):
        """CI regression prevention framework"""
        return CIParameterRegressionPrevention()

    def test_codebase_scanning_detects_issues(self, ci_framework):
        """Test that codebase scanning detects parameter compatibility issues"""

        # Create temporary test files with known issues
        test_content_with_issues = """
import openai

def bad_function():
    response = openai.chat.completions.create(
        model="gpt-5-nano",
        messages=[{"role": "user", "content": "test"}],
        max_tokens=150,  # Issue: max_tokens with GPT-5
        temperature=0.7   # Issue: temperature != 1.0 with GPT-5
    )
    return response
        """

        # Write test file
        test_file = ci_framework.repo_root / "test_issue_detection.py"
        test_file.write_text(test_content_with_issues)

        try:
            # Analyze the test file
            analysis = ci_framework.analyzer.analyze_file(test_file)

            # Validate issue detection
            assert analysis["issues_found"] >= 2, (
                f"Expected at least 2 issues, found {analysis['issues_found']}"
            )

            # Check specific issues
            issues = analysis["incompatible_patterns"]
            max_tokens_issue = any(
                issue["issue"] == "max_tokens_usage" for issue in issues
            )
            temperature_issue = any(
                issue["issue"] == "temperature_compatibility" for issue in issues
            )

            assert max_tokens_issue, "max_tokens issue not detected"
            assert temperature_issue, "Temperature compatibility issue not detected"

            print(
                f"‚úÖ Issue detection test passed: {analysis['issues_found']} issues detected"
            )

        finally:
            # Clean up test file
            if test_file.exists():
                test_file.unlink()

    def test_parameter_transformation_validation(self, ci_framework):
        """Test parameter transformation validation"""

        validation_results = ci_framework.validate_parameter_transformation_functions()

        # Validate that transformation functions work correctly
        assert validation_results["all_tests_passed"], (
            f"Parameter transformation validation failed: {validation_results['critical_failures']}"
        )

        # Check specific transformations
        test_results = {
            test["test_name"]: test
            for test in validation_results["transformation_tests"]
        }

        # Validate GPT-5 max_tokens transformation
        max_tokens_test = test_results.get("max_tokens_to_max_completion_tokens_gpt5")
        assert max_tokens_test and max_tokens_test["passed"], (
            "max_tokens to max_completion_tokens transformation failed"
        )

        # Validate temperature filtering
        temp_test = test_results.get("temperature_filtering_gpt5")
        assert temp_test and temp_test["passed"], (
            "Temperature filtering for GPT-5 failed"
        )

        print(
            f"‚úÖ Parameter transformation validation passed: {len(validation_results['transformation_tests'])} tests"
        )

    def test_deployment_blocking_logic(self, ci_framework):
        """Test that critical issues block deployment correctly"""

        # Mock analysis results with critical issues
        mock_codebase_scan = {
            "summary": {"critical_issues": 3, "blocks_deployment": True}
        }

        mock_transformation_validation = {
            "all_tests_passed": False,
            "critical_failures": ["max_tokens_transformation_failed"],
        }

        mock_git_analysis = {"risk_assessment": "HIGH"}

        # Test deployment blocking logic
        blocks_deployment = (
            mock_codebase_scan["summary"]["blocks_deployment"]
            or not mock_transformation_validation["all_tests_passed"]
            or mock_git_analysis["risk_assessment"] == "HIGH"
        )

        assert blocks_deployment, "Critical issues should block deployment"

        # Test passing scenario
        mock_codebase_scan["summary"] = {
            "critical_issues": 0,
            "blocks_deployment": False,
        }
        mock_transformation_validation["all_tests_passed"] = True
        mock_git_analysis["risk_assessment"] = "LOW"

        blocks_deployment_passing = (
            mock_codebase_scan["summary"]["blocks_deployment"]
            or not mock_transformation_validation["all_tests_passed"]
            or mock_git_analysis["risk_assessment"] == "HIGH"
        )

        assert not blocks_deployment_passing, (
            "No critical issues should allow deployment"
        )

        print("‚úÖ Deployment blocking logic validation passed")

    def test_git_change_analysis(self, ci_framework):
        """Test analysis of git changes for parameter impact"""

        git_analysis = ci_framework.check_git_changes_for_parameter_impact()

        # Validate git analysis structure
        assert "git_available" in git_analysis

        if git_analysis["git_available"]:
            assert "total_changed_files" in git_analysis
            assert "python_files_changed" in git_analysis
            assert "risk_assessment" in git_analysis

            # Validate risk assessment values
            valid_risk_levels = ["LOW", "MEDIUM", "HIGH"]
            assert git_analysis["risk_assessment"] in valid_risk_levels, (
                f"Invalid risk assessment: {git_analysis['risk_assessment']}"
            )

            print(
                f"‚úÖ Git change analysis: {git_analysis['python_files_changed']} Python files, {git_analysis['risk_assessment']} risk"
            )
        else:
            print("‚ö†Ô∏è  Git not available for change analysis")

    def test_comprehensive_ci_report_generation(self, ci_framework):
        """Test comprehensive CI report generation"""

        report = ci_framework.generate_ci_report()

        # Validate report structure
        required_keys = [
            "timestamp",
            "overall_status",
            "blocks_deployment",
            "analyses",
            "summary",
        ]
        for key in required_keys:
            assert key in report, f"Missing report key: {key}"

        # Validate status values
        assert report["overall_status"] in [
            "PASS",
            "FAIL",
        ], f"Invalid overall status: {report['overall_status']}"

        # Validate analyses structure
        analyses = report["analyses"]
        assert "codebase_scan" in analyses
        assert "transformation_validation" in analyses
        assert "git_analysis" in analyses

        # Validate summary
        summary = report["summary"]
        assert "recommendation" in summary
        assert isinstance(summary["critical_issues_found"], int)
        assert isinstance(summary["transformation_tests_passed"], bool)

        print(f"‚úÖ CI report generated: {report['overall_status']} status")
        print(f"   {summary['recommendation']}")

        return report

    def test_performance_regression_detection_in_ci(self):
        """Test performance regression detection for CI/CD"""

        # Mock performance metrics that would indicate regression
        performance_scenarios = [
            {
                "name": "no_regression",
                "metrics": {
                    "avg_response_ms": 850,
                    "success_rate": 0.97,
                    "cascade_rate": 0.01,
                },
                "should_block": False,
            },
            {
                "name": "response_time_regression",
                "metrics": {
                    "avg_response_ms": 15000,
                    "success_rate": 0.85,
                    "cascade_rate": 0.25,
                },
                "should_block": True,
            },
            {
                "name": "success_rate_regression",
                "metrics": {
                    "avg_response_ms": 1200,
                    "success_rate": 0.75,
                    "cascade_rate": 0.18,
                },
                "should_block": True,
            },
            {
                "name": "minor_degradation",
                "metrics": {
                    "avg_response_ms": 1800,
                    "success_rate": 0.92,
                    "cascade_rate": 0.05,
                },
                "should_block": False,
            },
        ]

        def should_block_deployment(metrics):
            """Determine if performance metrics should block deployment"""
            return (
                metrics["avg_response_ms"] > 5000  # 5+ second responses
                or metrics["success_rate"] < 0.9  # <90% success rate
                or metrics["cascade_rate"] > 0.15  # >15% cascade rate
            )

        for scenario in performance_scenarios:
            result = should_block_deployment(scenario["metrics"])

            assert result == scenario["should_block"], (
                f"Performance regression detection failed for {scenario['name']}: expected {scenario['should_block']}, got {result}"
            )

            if result:
                print(f"‚úÖ {scenario['name']}: Correctly blocked deployment")
            else:
                print(f"‚úÖ {scenario['name']}: Correctly allowed deployment")

    @pytest.mark.parametrize(
        "file_content,expected_issues",
        [
            # Test case 1: Clean code with proper parameter usage
            (
                """
def good_function():
    params = {
        "model": "gpt-5-nano",
        "max_completion_tokens": 150,
        "temperature": 1.0
    }
    return params
        """,
                0,
            ),
            # Test case 2: Code with max_tokens issue
            (
                """
def bad_function():
    return openai.chat.completions.create(
        model="gpt-5-nano",
        max_tokens=150
    )
        """,
                1,
            ),
            # Test case 3: Multiple parameter issues
            (
                """
def multiple_issues():
    params = {
        "model": "gpt-5",
        "max_tokens": 200,
        "temperature": 0.5,
        "top_p": 0.9
    }
    return params
        """,
                3,
            ),
        ],
    )
    def test_parameter_issue_detection_scenarios(
        self, ci_framework, file_content, expected_issues
    ):
        """Test various parameter issue detection scenarios"""

        # Write test content to temporary file
        test_file = ci_framework.repo_root / f"test_scenario_{expected_issues}.py"
        test_file.write_text(file_content)

        try:
            # Analyze the file
            analysis = ci_framework.analyzer.analyze_file(test_file)

            # Validate issue count
            assert analysis["issues_found"] == expected_issues, (
                f"Expected {expected_issues} issues, found {analysis['issues_found']} in content: {file_content[:100]}..."
            )

            print(
                f"‚úÖ Scenario test passed: {expected_issues} issues detected as expected"
            )

        finally:
            # Clean up
            if test_file.exists():
                test_file.unlink()


class TestCIIntegrationWorkflow:
    """Test CI/CD integration workflow for parameter compatibility"""

    def test_pr_validation_workflow(self):
        """Test the complete PR validation workflow"""

        # Simulate PR validation steps
        validation_steps = [
            {"name": "codebase_scan", "status": "PASS", "critical": True},
            {"name": "transformation_tests", "status": "PASS", "critical": True},
            {"name": "performance_regression", "status": "PASS", "critical": True},
            {"name": "git_change_analysis", "status": "WARN", "critical": False},
            {"name": "documentation_check", "status": "PASS", "critical": False},
        ]

        # Determine overall PR status
        critical_failures = [
            step
            for step in validation_steps
            if step["critical"] and step["status"] == "FAIL"
        ]
        overall_status = "FAIL" if critical_failures else "PASS"

        assert overall_status == "PASS", (
            f"PR validation failed due to critical failures: {critical_failures}"
        )

        # Check warning handling
        warnings = [step for step in validation_steps if step["status"] == "WARN"]
        assert len(warnings) <= 2, f"Too many warnings in PR validation: {warnings}"

        print(
            f"‚úÖ PR validation workflow: {overall_status} with {len(warnings)} warnings"
        )

    def test_deployment_gate_logic(self):
        """Test deployment gate logic for parameter compatibility"""

        # Define deployment gate criteria
        deployment_gates = {
            "parameter_compatibility_scan": {"required": True, "weight": 1.0},
            "transformation_function_tests": {"required": True, "weight": 1.0},
            "performance_regression_check": {"required": True, "weight": 0.8},
            "git_security_analysis": {"required": False, "weight": 0.3},
        }

        # Test scenarios
        test_scenarios = [
            {
                "name": "all_gates_pass",
                "gate_results": {
                    "parameter_compatibility_scan": "PASS",
                    "transformation_function_tests": "PASS",
                    "performance_regression_check": "PASS",
                    "git_security_analysis": "PASS",
                },
                "should_deploy": True,
            },
            {
                "name": "required_gate_fails",
                "gate_results": {
                    "parameter_compatibility_scan": "FAIL",
                    "transformation_function_tests": "PASS",
                    "performance_regression_check": "PASS",
                    "git_security_analysis": "PASS",
                },
                "should_deploy": False,
            },
            {
                "name": "optional_gate_fails",
                "gate_results": {
                    "parameter_compatibility_scan": "PASS",
                    "transformation_function_tests": "PASS",
                    "performance_regression_check": "PASS",
                    "git_security_analysis": "FAIL",
                },
                "should_deploy": True,
            },
        ]

        for scenario in test_scenarios:
            # Calculate deployment decision
            can_deploy = True

            for gate_name, gate_config in deployment_gates.items():
                gate_result = scenario["gate_results"].get(gate_name, "UNKNOWN")

                if gate_config["required"] and gate_result == "FAIL":
                    can_deploy = False
                    break

            assert can_deploy == scenario["should_deploy"], (
                f"Deployment gate logic failed for {scenario['name']}: expected {scenario['should_deploy']}, got {can_deploy}"
            )

            print(f"‚úÖ {scenario['name']}: Deployment decision correct ({can_deploy})")

    def test_rollback_trigger_conditions(self):
        """Test conditions that should trigger automatic rollback"""

        # Define rollback trigger conditions
        rollback_conditions = [
            {"metric": "cascade_rate", "threshold": 0.10, "window_minutes": 5},
            {"metric": "avg_response_time_ms", "threshold": 10000, "window_minutes": 3},
            {
                "metric": "success_rate",
                "threshold": 0.85,
                "comparison": "below",
                "window_minutes": 2,
            },
            {"metric": "error_rate", "threshold": 0.05, "window_minutes": 1},
        ]

        # Test production metrics that should trigger rollback
        production_metrics = [
            {
                "scenario": "cascade_spike",
                "metrics": {
                    "cascade_rate": 0.25,
                    "avg_response_time_ms": 15000,
                    "success_rate": 0.70,
                },
                "should_rollback": True,
                "triggered_by": [
                    "cascade_rate",
                    "avg_response_time_ms",
                    "success_rate",
                ],
            },
            {
                "scenario": "normal_operation",
                "metrics": {
                    "cascade_rate": 0.02,
                    "avg_response_time_ms": 900,
                    "success_rate": 0.97,
                },
                "should_rollback": False,
                "triggered_by": [],
            },
            {
                "scenario": "minor_degradation",
                "metrics": {
                    "cascade_rate": 0.05,
                    "avg_response_time_ms": 1800,
                    "success_rate": 0.92,
                },
                "should_rollback": False,
                "triggered_by": [],
            },
        ]

        for scenario_data in production_metrics:
            triggered_conditions = []
            metrics = scenario_data["metrics"]

            # Check each rollback condition
            for condition in rollback_conditions:
                metric_name = condition["metric"]
                threshold = condition["threshold"]
                comparison = condition.get("comparison", "above")

                if metric_name in metrics:
                    metric_value = metrics[metric_name]

                    if comparison == "above" and metric_value > threshold:
                        triggered_conditions.append(metric_name)
                    elif comparison == "below" and metric_value < threshold:
                        triggered_conditions.append(metric_name)

            should_rollback = len(triggered_conditions) > 0

            assert should_rollback == scenario_data["should_rollback"], (
                f"Rollback logic failed for {scenario_data['scenario']}: expected {scenario_data['should_rollback']}, got {should_rollback}"
            )

            print(
                f"‚úÖ {scenario_data['scenario']}: Rollback decision correct ({'ROLLBACK' if should_rollback else 'CONTINUE'})"
            )
            if triggered_conditions:
                print(f"   Triggered by: {triggered_conditions}")


# CI/CD Integration Command Line Interface
if __name__ == "__main__":
    import argparse
    import sys

    parser = argparse.ArgumentParser(
        description="OpenAI Parameter Compatibility CI/CD Validation"
    )
    parser.add_argument(
        "--scan-only", action="store_true", help="Only scan codebase for issues"
    )
    parser.add_argument(
        "--validate-transforms",
        action="store_true",
        help="Only validate parameter transformations",
    )
    parser.add_argument(
        "--check-git", action="store_true", help="Only check git changes"
    )
    parser.add_argument(
        "--full-report", action="store_true", help="Generate full CI/CD report"
    )
    parser.add_argument("--output", help="Output file for results (JSON)")

    args = parser.parse_args()

    ci_framework = CIParameterRegressionPrevention()

    if args.scan_only:
        results = ci_framework.scan_codebase_for_parameter_issues()
        print(f"\nüìã Codebase Scan Results:")
        print(f"Files scanned: {results['summary']['files_scanned']}")
        print(f"Critical issues: {results['summary']['critical_issues']}")
        print(f"Blocks deployment: {results['summary']['blocks_deployment']}")

        if args.output:
            with open(args.output, "w") as f:
                json.dump(results, f, indent=2)

        sys.exit(1 if results["summary"]["blocks_deployment"] else 0)

    elif args.validate_transforms:
        results = ci_framework.validate_parameter_transformation_functions()
        print(f"\nüîß Parameter Transformation Validation:")
        print(f"All tests passed: {results['all_tests_passed']}")

        if args.output:
            with open(args.output, "w") as f:
                json.dump(results, f, indent=2)

        sys.exit(0 if results["all_tests_passed"] else 1)

    elif args.check_git:
        results = ci_framework.check_git_changes_for_parameter_impact()
        print(f"\nüìù Git Change Analysis:")
        print(f"Risk level: {results.get('risk_assessment', 'UNKNOWN')}")

        if args.output:
            with open(args.output, "w") as f:
                json.dump(results, f, indent=2)

        sys.exit(1 if results.get("risk_assessment") == "HIGH" else 0)

    else:  # Full report or default
        report = ci_framework.generate_ci_report()
        print(f"\nüìä CI/CD Parameter Compatibility Report:")
        print(f"Overall status: {report['overall_status']}")
        print(f"Blocks deployment: {report['blocks_deployment']}")
        print(f"Recommendation: {report['summary']['recommendation']}")

        if args.output:
            with open(args.output, "w") as f:
                json.dump(report, f, indent=2)

        sys.exit(1 if report["blocks_deployment"] else 0)
